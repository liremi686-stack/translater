import os
import warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
import re
import json
import requests
import numpy as np
import pandas as pd
from collections import Counter, defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import threading
from datetime import datetime
import pickle
import sqlite3
from langdetect import detect, DetectorFactory
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer, AutoModel, pipeline, MarianMTModel, MarianTokenizer
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import gensim
from gensim.models import Word2Vec, FastText
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import logging
from googletrans import Translator as GoogleTranslator
import zipfile
import tarfile
import urllib.request
from pathlib import Path

# –î–ª—è consistent results
DetectorFactory.seed = 0
# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
except:
    print("‚ö†Ô∏è NLTK —Ä–µ—Å—É—Ä—Å—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

class TatoebaDataset:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∫–æ—Ä–ø—É—Å–∞–º–∏ Tatoeba"""
    
    def __init__(self, data_dir="tatoeba_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.download_urls = {
            'sentences': 'https://downloads.tatoeba.org/exports/sentences.csv',
            'links': 'https://downloads.tatoeba.org/exports/links.csv',
            'translations': 'https://downloads.tatoeba.org/exports/links.csv'
        }
    
    def download_dataset(self, progress_callback=None):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ Tatoeba —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        print("üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ Tatoeba...")
        
        total_files = len(self.download_urls)
        for i, (name, url) in enumerate(self.download_urls.items()):
            if progress_callback:
                progress_callback((i / total_files) * 100, f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {name}.csv...")
                
            file_path = self.data_dir / f"{name}.csv"
            if not file_path.exists():
                try:
                    urllib.request.urlretrieve(url, file_path)
                    print(f"‚úÖ {name} —Å–∫–∞—á–∞–Ω")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {name}: {e}")
        
        if progress_callback:
            progress_callback(100, "–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    def load_parallel_corpus(self, source_lang='eng', target_lang='rus', max_samples=50000, progress_callback=None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            if progress_callback:
                progress_callback(0, "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤...")
                
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences_df = pd.read_csv(
                self.data_dir / "sentences.csv", 
                sep='\t', 
                names=['id', 'lang', 'text'],
                usecols=[0, 1, 2]
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤—è–∑–∏ (–ø–µ—Ä–µ–≤–æ–¥—ã)
            links_df = pd.read_csv(
                self.data_dir / "links.csv",
                sep='\t',
                names=['source_id', 'target_id']
            )
            
            if progress_callback:
                progress_callback(30, "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —è–∑—ã–∫–∞–º...")
                
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —è–∑—ã–∫–∞–º
            source_sentences = sentences_df[sentences_df['lang'] == source_lang]
            target_sentences = sentences_df[sentences_df['lang'] == target_lang]
            
            if progress_callback:
                progress_callback(50, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π...")
                
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            source_dict = dict(zip(source_sentences['id'], source_sentences['text']))
            target_dict = dict(zip(target_sentences['id'], target_sentences['text']))
            
            if progress_callback:
                progress_callback(70, "–°–±–æ—Ä–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–∞—Ä...")
                
            # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã
            parallel_pairs = []
            total_links = min(len(links_df), max_samples * 3)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            
            for idx, (_, row) in enumerate(links_df.iterrows()):
                if len(parallel_pairs) >= max_samples:
                    break
                    
                source_id = row['source_id']
                target_id = row['target_id']
                
                if source_id in source_dict and target_id in target_dict:
                    parallel_pairs.append({
                        'source': source_dict[source_id],
                        'target': target_dict[target_id],
                        'source_lang': source_lang,
                        'target_lang': target_lang
                    })
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 –∏—Ç–µ—Ä–∞—Ü–∏–π
                if progress_callback and idx % 1000 == 0:
                    progress = 70 + (idx / total_links) * 25
                    progress_callback(progress, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(parallel_pairs)} –ø–∞—Ä...")
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(parallel_pairs)} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            
            if progress_callback:
                progress_callback(100, "–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                
            return parallel_pairs
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ—Ä–ø—É—Å–∞: {e}")
            if progress_callback:
                progress_callback(0, f"–û—à–∏–±–∫–∞: {e}")
            return []
    
    def get_available_languages(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤"""
        try:
            sentences_df = pd.read_csv(
                self.data_dir / "sentences.csv", 
                sep='\t', 
                names=['id', 'lang', 'text'],
                usecols=[0, 1, 2]
            )
            return sorted(sentences_df['lang'].unique())
        except:
            return ['eng', 'fra', 'deu', 'spa', 'rus', 'ita', 'por', 'jpn', 'kor', 'cmn']

class ProgressWindow:
    """–û–∫–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    
    def __init__(self, parent, title="–ü—Ä–æ–≥—Ä–µ—Å—Å"):
        self.window = tk.Toplevel(parent)
        self.window.title(title)
        self.window.geometry("400x150")
        self.window.transient(parent)
        self.window.grab_set()
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫–Ω–∞
        self.window.update_idletasks()
        x = parent.winfo_x() + (parent.winfo_width() - self.window.winfo_width()) // 2
        y = parent.winfo_y() + (parent.winfo_height() - self.window.winfo_height()) // 2
        self.window.geometry(f"+{x}+{y}")
        
        # –≠–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        self.label = ttk.Label(self.window, text="–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–ø–µ—Ä–∞—Ü–∏—è...")
        self.label.pack(pady=10)
        
        self.progress = ttk.Progressbar(self.window, orient="horizontal", length=350, mode="determinate")
        self.progress.pack(pady=10)
        
        self.detail_label = ttk.Label(self.window, text="")
        self.detail_label.pack(pady=5)
        
        self.cancel_button = ttk.Button(self.window, text="–û—Ç–º–µ–Ω–∞", command=self.cancel)
        self.cancel_button.pack(pady=5)
        
        self.is_cancelled = False
        
    def update(self, value, text=""):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress['value'] = value
        if text:
            self.detail_label.config(text=text)
        self.window.update()
        
    def cancel(self):
        """–û—Ç–º–µ–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        self.is_cancelled = True
        self.window.destroy()
        
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –æ–∫–Ω–∞"""
        self.window.destroy()

class AdvancedNeuralTranslator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
    
    def __init__(self, source_vocab_size=30000, target_vocab_size=30000, embed_size=256, hidden_size=512):
        self.source_vocab_size = source_vocab_size
        self.target_vocab_size = target_vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        
        # –≠–Ω–∫–æ–¥–µ—Ä
        self.encoder_embedding = nn.Embedding(source_vocab_size, embed_size)
        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)
        
        # –î–µ–∫–æ–¥–µ—Ä
        self.decoder_embedding = nn.Embedding(target_vocab_size, embed_size)
        self.decoder_lstm = nn.LSTM(embed_size + hidden_size * 2, hidden_size, batch_first=True)
        
        # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        self.attention = nn.MultiheadAttention(hidden_size * 2, num_heads=8, batch_first=True)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.fc_out = nn.Linear(hidden_size, target_vocab_size)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, source, target):
        # –≠–Ω–∫–æ–¥–∏–Ω–≥
        source_embedded = self.encoder_embedding(source)
        encoder_output, (hidden, cell) = self.encoder_lstm(source_embedded)
        
        # –î–µ–∫–æ–¥–∏–Ω–≥ —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º
        target_embedded = self.decoder_embedding(target)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ
        attn_output, _ = self.attention(
            target_embedded, encoder_output, encoder_output
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Ü–µ–ª–∏
        decoder_input = torch.cat([target_embedded, attn_output], dim=-1)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
        decoder_output, _ = self.decoder_lstm(decoder_input)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        output = self.fc_out(self.dropout(decoder_output))
        
        return output

class TransformerTranslator:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-ru'):
        self.model_name = model_name
        self.setup_model()
    
    def setup_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)
            self.model = MarianMTModel.from_pretrained(self.model_name)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {self.model_name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {self.model_name}: {e}")
            self.model = None
            self.tokenizer = None
    
    def translate(self, text, max_length=512):
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
        if self.model is None or self.tokenizer is None:
            return f"–ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞
            with torch.no_grad():
                translated = self.model.generate(**inputs)
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            translation = self.tokenizer.decode(translated[0], skip_special_tokens=True)
            return translation
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}"

class ImprovedEnhancedTranslator:
    def __init__(self):
        self.tatoeba = TatoebaDataset()
        self.transformer_models = {}
        self.neural_model = None
        self.setup_models()
    
    def detect_language_pair(self, text, target_lang='ru'):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏"""
        try:
            source_lang = detect(text)
            lang_map = {
                'en': 'en', 'de': 'de', 'fr': 'fr', 
                'es': 'es', 'zh': 'zh', 'ru': 'ru'
            }
            
            source_code = lang_map.get(source_lang, 'en')
            target_code = lang_map.get(target_lang, 'ru')
            
            model_name = f'Helsinki-NLP/opus-mt-{source_code}-{target_code}'
            if model_name in self.transformer_models:
                return model_name
            else:
                # –ü—Ä–æ–±—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é –ø–∞—Ä—É
                reverse_model = f'Helsinki-NLP/opus-mt-{target_code}-{source_code}'
                if reverse_model in self.transformer_models:
                    return reverse_model
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –ø–∞—Ä—É
            return 'Helsinki-NLP/opus-mt-en-ru'
            
        except:
            return 'Helsinki-NLP/opus-mt-en-ru'
    
    def translate_with_transformers(self, text, target_lang='ru'):
        """–ü–µ—Ä–µ–≤–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        model_name = self.detect_language_pair(text, target_lang)
        translator = self.transformer_models.get(model_name)
        
        if translator:
            return translator.translate(text)
        else:
            return f"–ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –¢–µ–∫—Å—Ç: {text}"
    
    def download_tatoeba_corpus(self, source_lang='eng', target_lang='rus', progress_callback=None):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ Tatoeba —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ {source_lang}-{target_lang}...")
        self.tatoeba.download_dataset(progress_callback)
        return self.tatoeba.load_parallel_corpus(source_lang, target_lang, progress_callback=progress_callback)
    
    def train_custom_model(self, source_texts, target_texts, epochs=5, batch_size=32):
        """–û–±—É—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π
            source_vocab = self._build_vocabulary(source_texts, self.source_vocab_size)
            target_vocab = self._build_vocabulary(target_texts, self.target_vocab_size)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            self.neural_model = AdvancedNeuralTranslator(
                source_vocab_size=len(source_vocab),
                target_vocab_size=len(target_vocab)
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            optimizer = optim.Adam(self.neural_model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss(ignore_index=0)
            
            losses = []
            for epoch in range(epochs):
                epoch_loss = 0
                for i in range(0, len(source_texts), batch_size):
                    batch_source = source_texts[i:i+batch_size]
                    batch_target = target_texts[i:i+batch_size]
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
                    source_tensor = self._texts_to_tensor(batch_source, source_vocab)
                    target_tensor = self._texts_to_tensor(batch_target, target_vocab)
                    
                    optimizer.zero_grad()
                    output = self.neural_model(source_tensor, target_tensor[:, :-1])
                    
                    loss = criterion(
                        output.reshape(-1, output.shape[-1]),
                        target_tensor[:, 1:].reshape(-1)
                    )
                    
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_loss = epoch_loss / (len(source_texts) / batch_size)
                losses.append(avg_loss)
                print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
            
            return losses
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return []
    
    def _build_vocabulary(self, texts, max_vocab_size=30000):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
        word_counts = Counter()
        for text in texts:
            words = word_tokenize(text.lower())
            word_counts.update(words)
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}
        
        for i, (word, count) in enumerate(word_counts.most_common(max_vocab_size - 4)):
            vocab[word] = i + 4
        
        return vocab
    
    def _texts_to_tensor(self, texts, vocab):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã"""
        tensors = []
        for text in texts:
            words = ['<SOS>'] + word_tokenize(text.lower()) + ['<EOS>']
            indices = [vocab.get(word, vocab['<UNK>']) for word in words]
            tensors.append(torch.tensor(indices, dtype=torch.long))
        
        # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        max_len = max(len(t) for t in tensors)
        padded_tensors = []
        
        for tensor in tensors:
            padding = torch.zeros(max_len - len(tensor), dtype=torch.long)
            padded_tensors.append(torch.cat([tensor, padding]))
        
        return torch.stack(padded_tensors)

    def setup_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        language_pairs = [
            'Helsinki-NLP/opus-mt-en-ru',
            'Helsinki-NLP/opus-mt-en-de', 
            'Helsinki-NLP/opus-mt-en-fr',
            'Helsinki-NLP/opus-mt-en-es',
            'Helsinki-NLP/opus-mt-en-zh',
            'Helsinki-NLP/opus-mt-ru-en'
        ]
    
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    
        for model_name in language_pairs:
            try:
                print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ {model_name}...")
                self.transformer_models[model_name] = TransformerTranslator(model_name)
                print(f"‚úÖ {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –º–æ–¥–µ–ª—å, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏
    
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.transformer_models)} –º–æ–¥–µ–ª–µ–π")

class TextWidgetWithMenu(scrolledtext.ScrolledText):
    """–¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –º–µ–Ω—é"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.create_context_menu()
        
    def create_context_menu(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–µ–Ω—é"""
        self.context_menu = tk.Menu(self, tearoff=0)
        self.context_menu.add_command(label="–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å", command=self.copy_text)
        self.context_menu.add_command(label="–í—Å—Ç–∞–≤–∏—Ç—å", command=self.paste_text)
        self.context_menu.add_command(label="–í—ã—Ä–µ–∑–∞—Ç—å", command=self.cut_text)
        self.context_menu.add_separator()
        self.context_menu.add_command(label="–í—ã–¥–µ–ª–∏—Ç—å –≤—Å–µ", command=self.select_all)
        
        # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤—É—é –∫–Ω–æ–ø–∫—É –º—ã—à–∏
        self.bind("<Button-3>", self.show_context_menu)
        
    def show_context_menu(self, event):
        """–ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –º–µ–Ω—é"""
        self.context_menu.tk_popup(event.x_root, event.y_root)
        
    def copy_text(self):
        """–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        try:
            self.clipboard_clear()
            text = self.get(tk.SEL_FIRST, tk.SEL_LAST)
            self.clipboard_append(text)
        except tk.TclError:
            pass  # –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–¥–µ–ª–µ–Ω–æ
            
    def paste_text(self):
        """–í—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç"""
        try:
            text = self.selection_get(selection='CLIPBOARD')
            self.insert(tk.INSERT, text)
        except tk.TclError:
            pass  # –ë—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞ –ø—É—Å—Ç
            
    def cut_text(self):
        """–í—ã—Ä–µ–∑–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        try:
            self.copy_text()
            self.delete(tk.SEL_FIRST, tk.SEL_LAST)
        except tk.TclError:
            pass  # –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–¥–µ–ª–µ–Ω–æ
            
    def select_all(self):
        """–í—ã–¥–µ–ª–∏—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç"""
        self.tag_add(tk.SEL, "1.0", tk.END)
        self.mark_set(tk.INSERT, "1.0")
        self.see(tk.INSERT)

class EnhancedTranslationGUI:
    def __init__(self):
        self.translator = ImprovedEnhancedTranslator()
        self.root = tk.Tk()
        self.setup_gui()
        
    def setup_gui(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        self.root.title("üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å Tatoeba –∏ Neural Models")
        self.root.geometry("1000x800")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫
        notebook = ttk.Notebook(self.root)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
        tabs = {
            "üìñ –ü–µ—Ä–µ–≤–æ–¥": self.setup_translation_tab,
            "üß† –ù–µ–π—Ä–æ—Å–µ—Ç–∏": self.setup_neural_tab,
            "üìö Tatoeba": self.setup_tatoeba_tab,
            "üìä –ö–∞—á–µ—Å—Ç–≤–æ": self.setup_quality_tab,
            "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏": self.setup_settings_tab
        }
        
        for tab_name, setup_func in tabs.items():
            frame = ttk.Frame(notebook)
            setup_func(frame)
            notebook.add(frame, text=tab_name)
        
        notebook.pack(expand=True, fill='both', padx=10, pady=10)
        
    def setup_translation_tab(self, parent):
        """–í–∫–ª–∞–¥–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        model_frame = ttk.LabelFrame(parent, text="–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞")
        model_frame.pack(fill='x', padx=10, pady=5)
        
        self.model_var = tk.StringVar(value="transformer")
        ttk.Radiobutton(model_frame, text="ü§ñ Transformer (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)", 
                       variable=self.model_var, value="transformer").pack(anchor='w')
        ttk.Radiobutton(model_frame, text="üß† Neural LSTM", 
                       variable=self.model_var, value="neural").pack(anchor='w')
        ttk.Radiobutton(model_frame, text="üîß –ê–Ω—Å–∞–º–±–ª—å", 
                       variable=self.model_var, value="ensemble").pack(anchor='w')
        
        # –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        lang_frame = ttk.LabelFrame(parent, text="–Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        lang_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Label(lang_frame, text="–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫:").grid(row=0, column=0, sticky='w')
        self.target_lang = ttk.Combobox(lang_frame, values=['ru', 'en', 'de', 'fr', 'es', 'zh'])
        self.target_lang.set('ru')
        self.target_lang.grid(row=0, column=1, sticky='w', padx=5)
        
        # –ü–æ–ª–µ –≤–≤–æ–¥–∞
        input_frame = ttk.LabelFrame(parent, text="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
        input_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.input_text = TextWidgetWithMenu(input_frame, height=10, wrap=tk.WORD)
        self.input_text.pack(fill='both', expand=True, padx=10, pady=10)
        
        # –ö–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∞
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(button_frame, text="üöÄ –ü–µ—Ä–µ–≤–µ—Å—Ç–∏", 
                  command=self.translate_text).pack(side='left', padx=5)
        ttk.Button(button_frame, text="üîÑ –û—á–∏—Å—Ç–∏—Ç—å", 
                  command=self.clear_text).pack(side='left', padx=5)
        ttk.Button(button_frame, text="üìã –í—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä", 
                  command=self.insert_example).pack(side='left', padx=5)
        
        # –ü–æ–ª–µ –≤—ã–≤–æ–¥–∞
        output_frame = ttk.LabelFrame(parent, text="–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞")
        output_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.output_text = TextWidgetWithMenu(output_frame, height=10, wrap=tk.WORD)
        self.output_text.pack(fill='both', expand=True, padx=10, pady=10)
        
        # –ö–Ω–æ–ø–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        result_buttons = ttk.Frame(parent)
        result_buttons.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(result_buttons, text="üìã –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥", 
                  command=self.copy_translation).pack(side='left', padx=5)
        ttk.Button(result_buttons, text="üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª", 
                  command=self.save_translation).pack(side='left', padx=5)
        
        # –°—Ç–∞—Ç—É—Å
        self.status_label = ttk.Label(parent, text="–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        self.status_label.pack(pady=5)
    
    def setup_neural_tab(self, parent):
        """–í–∫–ª–∞–¥–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        ttk.Label(parent, text="–û–±—É—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data_frame = ttk.LabelFrame(parent, text="–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        data_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(data_frame, text="üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å", 
                  command=self.load_training_data).pack(pady=5)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        train_frame = ttk.LabelFrame(parent, text="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
        train_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Label(train_frame, text="–≠–ø–æ—Ö–∏:").grid(row=0, column=0, sticky='w')
        self.epochs_entry = ttk.Entry(train_frame, width=10)
        self.epochs_entry.insert(0, "5")
        self.epochs_entry.grid(row=0, column=1, sticky='w', padx=5)
        
        ttk.Label(train_frame, text="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:").grid(row=0, column=2, sticky='w')
        self.batch_entry = ttk.Entry(train_frame, width=10)
        self.batch_entry.insert(0, "32")
        self.batch_entry.grid(row=0, column=3, sticky='w', padx=5)
        
        # –ö–Ω–æ–ø–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        train_buttons = ttk.Frame(parent)
        train_buttons.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(train_buttons, text="üéì –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", 
                  command=self.train_neural_model).pack(side='left', padx=5)
        ttk.Button(train_buttons, text="üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å", 
                  command=self.save_model).pack(side='left', padx=5)
        ttk.Button(train_buttons, text="üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", 
                  command=self.load_model).pack(side='left', padx=5)
        
        # –õ–æ–≥ –æ–±—É—á–µ–Ω–∏—è
        log_frame = ttk.LabelFrame(parent, text="–õ–æ–≥ –æ–±—É—á–µ–Ω–∏—è")
        log_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.training_log = TextWidgetWithMenu(log_frame, height=15, wrap=tk.WORD)
        self.training_log.pack(fill='both', expand=True, padx=10, pady=10)
    
    def setup_tatoeba_tab(self, parent):
        """–í–∫–ª–∞–¥–∫–∞ —Ä–∞–±–æ—Ç—ã —Å Tatoeba"""
        ttk.Label(parent, text="–†–∞–±–æ—Ç–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∫–æ—Ä–ø—É—Å–∞–º–∏ Tatoeba", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
    
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞
        corpus_frame = ttk.LabelFrame(parent, text="–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞")
        corpus_frame.pack(fill='x', padx=10, pady=5)
    
        # –í–µ—Ä—Ö–Ω—è—è —Å—Ç—Ä–æ–∫–∞ —Å —è–∑—ã–∫–∞–º–∏
        lang_row = ttk.Frame(corpus_frame)
        lang_row.pack(fill='x', padx=10, pady=5)
    
        # –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫
        source_frame = ttk.Frame(lang_row)
        source_frame.pack(side='left', padx=10)
        ttk.Label(source_frame, text="–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫:").pack(side='left')
        self.source_lang_combo = ttk.Combobox(source_frame, values=['eng', 'fra', 'deu', 'spa', 'rus'], width=10)
        self.source_lang_combo.set('eng')
        self.source_lang_combo.pack(side='left', padx=5)
    
        # –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫
        target_frame = ttk.Frame(lang_row)
        target_frame.pack(side='left', padx=10)
        ttk.Label(target_frame, text="–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫:").pack(side='left')
        self.target_lang_combo = ttk.Combobox(target_frame, values=['eng', 'fra', 'deu', 'spa', 'rus'], width=10)
        self.target_lang_combo.set('rus')
        self.target_lang_combo.pack(side='left', padx=5)
    
        # –ö–Ω–æ–ø–∫–∞
        ttk.Button(corpus_frame, text="üì• –°–∫–∞—á–∞—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ—Ä–ø—É—Å", 
                command=self.download_tatoeba).pack(pady=10)
    
        # –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö
        data_frame = ttk.LabelFrame(parent, text="–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
        data_frame.pack(fill='both', expand=True, padx=10, pady=5)
    
        # –¢–∞–±–ª–∏—Ü–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        columns = ('source', 'target')
        self.corpus_tree = ttk.Treeview(data_frame, columns=columns, show='headings', height=10)
    
        self.corpus_tree.heading('source', text='–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç')
        self.corpus_tree.heading('target', text='–ü–µ—Ä–µ–≤–æ–¥')
    
        self.corpus_tree.column('source', width=400)
        self.corpus_tree.column('target', width=400)
    
        scrollbar = ttk.Scrollbar(data_frame, orient='vertical', command=self.corpus_tree.yview)
        self.corpus_tree.configure(yscrollcommand=scrollbar.set)
    
        self.corpus_tree.pack(side='left', fill='both', expand=True)
        scrollbar.pack(side='right', fill='y')
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –º–µ–Ω—é –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        self.tree_context_menu = tk.Menu(self.corpus_tree, tearoff=0)
        self.tree_context_menu.add_command(label="–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç", command=self.copy_source_text)
        self.tree_context_menu.add_command(label="–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥", command=self.copy_target_text)
        self.corpus_tree.bind("<Button-3>", self.show_tree_context_menu)
    
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_frame = ttk.LabelFrame(parent, text="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞")
        stats_frame.pack(fill='x', padx=10, pady=5)
    
        self.corpus_stats = ttk.Label(stats_frame, text="–ö–æ—Ä–ø—É—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        self.corpus_stats.pack(pady=5)
    
    def setup_quality_tab(self, parent):
        """–í–∫–ª–∞–¥–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        ttk.Label(parent, text="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
        
        # –û—Ü–µ–Ω–∫–∞
        eval_frame = ttk.LabelFrame(parent, text="–û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞")
        eval_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(eval_frame, text="üìù –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ", 
                  command=self.evaluate_quality).pack(pady=5)
        ttk.Button(eval_frame, text="üìä –°—Ä–∞–≤–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏", 
                  command=self.compare_models).pack(pady=5)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_frame = ttk.LabelFrame(parent, text="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
        results_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.quality_text = TextWidgetWithMenu(results_frame, height=20, wrap=tk.WORD)
        self.quality_text.pack(fill='both', expand=True, padx=10, pady=10)
    
    def setup_settings_tab(self, parent):
        """–í–∫–ª–∞–¥–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        ttk.Label(parent, text="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã", 
                 font=('Arial', 12, 'bold')).pack(pady=10)
        
        # –ú–æ–¥–µ–ª–∏
        model_frame = ttk.LabelFrame(parent, text="–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏")
        model_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(model_frame, text="üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏", 
                  command=self.update_models).pack(pady=5)
        ttk.Button(model_frame, text="üßπ –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –º–æ–¥–µ–ª–µ–π", 
                  command=self.clear_cache).pack(pady=5)
        
        # –õ–æ–≥–∏
        log_frame = ttk.LabelFrame(parent, text="–õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã")
        log_frame.pack(fill='both', expand=True, padx=10, pady=5)
        
        self.system_log = TextWidgetWithMenu(log_frame, height=15, wrap=tk.WORD)
        self.system_log.pack(fill='both', expand=True, padx=10, pady=10)
        
        ttk.Button(parent, text="üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –ª–æ–≥–∏", 
                  command=self.show_logs).pack(pady=5)
    
    def show_tree_context_menu(self, event):
        """–ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –º–µ–Ω—é –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã"""
        item = self.corpus_tree.identify_row(event.y)
        if item:
            self.corpus_tree.selection_set(item)
            self.tree_context_menu.tk_popup(event.x_root, event.y_root)
    
    def copy_source_text(self):
        """–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
        item = self.corpus_tree.selection()[0]
        values = self.corpus_tree.item(item, 'values')
        self.root.clipboard_clear()
        self.root.clipboard_append(values[0])
    
    def copy_target_text(self):
        """–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
        item = self.corpus_tree.selection()[0]
        values = self.corpus_tree.item(item, 'values')
        self.root.clipboard_clear()
        self.root.clipboard_append(values[1])
    
    def insert_example(self):
        """–í—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
        example_text = """Hello! This is an example text for translation. 
The advanced translator supports multiple languages and uses state-of-the-art transformer models 
to provide high-quality translations."""
        
        self.input_text.delete("1.0", tk.END)
        self.input_text.insert("1.0", example_text)
    
    def copy_translation(self):
        """–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞"""
        translation = self.output_text.get("1.0", tk.END).strip()
        if translation:
            self.root.clipboard_clear()
            self.root.clipboard_append(translation)
            self.status_label.config(text="–ü–µ—Ä–µ–≤–æ–¥ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞")
    
    def save_translation(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥ –≤ —Ñ–∞–π–ª"""
        translation = self.output_text.get("1.0", tk.END).strip()
        if not translation:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–ù–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        file_path = filedialog.asksaveasfilename(
            defaultextension=".txt",
            filetypes=[("Text files", "*.txt"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(translation)
                messagebox.showinfo("–£—Å–ø–µ—Ö", "–ü–µ—Ä–µ–≤–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª")
            except Exception as e:
                messagebox.showerror("–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
    
    def translate_text(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        text = self.input_text.get("1.0", tk.END).strip()
        if not text:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
            return
        
        self.status_label.config(text="–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–µ—Ä–µ–≤–æ–¥...")
        
        threading.Thread(target=self._translate_thread, args=(text,), daemon=True).start()
    
    def _translate_thread(self, text):
        """–ü–æ—Ç–æ–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
        try:
            target_lang = self.target_lang.get()
            method = self.model_var.get()
            
            if method == "transformer":
                result = self.translator.translate_with_transformers(text, target_lang)
            elif method == "neural":
                result = "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ"
            else:  # ensemble
                result = self.translator.translate_with_transformers(text, target_lang)
            
            self.root.after(0, self._show_translation, result)
            
        except Exception as e:
            self.root.after(0, self._show_error, f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}")
    
    def _show_translation(self, result):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞"""
        self.output_text.delete("1.0", tk.END)
        self.output_text.insert("1.0", result)
        self.status_label.config(text="–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    def _show_error(self, error):
        """–ü–æ–∫–∞–∑–∞—Ç—å –æ—à–∏–±–∫—É"""
        messagebox.showerror("–û—à–∏–±–∫–∞", error)
        self.status_label.config(text="–û—à–∏–±–∫–∞")
    
    def clear_text(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        self.input_text.delete("1.0", tk.END)
        self.output_text.delete("1.0", tk.END)
        self.status_label.config(text="–¢–µ–∫—Å—Ç –æ—á–∏—â–µ–Ω")
    
    def download_tatoeba(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ Tatoeba —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        source_lang = self.source_lang_combo.get()
        target_lang = self.target_lang_combo.get()
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.progress_window = ProgressWindow(self.root, "–ó–∞–≥—Ä—É–∑–∫–∞ Tatoeba")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        threading.Thread(target=self._download_tatoeba_thread, 
                        args=(source_lang, target_lang), daemon=True).start()
    
    def _download_tatoeba_thread(self, source_lang, target_lang):
        """–ü–æ—Ç–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è Tatoeba —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            def update_progress(value, text):
                if hasattr(self, 'progress_window') and self.progress_window:
                    self.root.after(0, self.progress_window.update, value, text)
            
            corpus = self.translator.download_tatoeba_corpus(
                source_lang, target_lang, progress_callback=update_progress
            )
            
            self.root.after(0, self._show_corpus_stats, corpus)
            self.root.after(0, lambda: self.training_log.insert(
                tk.END, f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(corpus)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n"
            ))
            
        except Exception as e:
            self.root.after(0, lambda: self.training_log.insert(
                tk.END, f"‚ùå –û—à–∏–±–∫–∞: {str(e)}\n"
            ))
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –æ–∫–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if hasattr(self, 'progress_window') and self.progress_window:
                self.root.after(0, self.progress_window.close)
    
    def _show_corpus_stats(self, corpus):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ—Ä–ø—É—Å–∞"""
        if not corpus:
            self.corpus_stats.config(text="–ö–æ—Ä–ø—É—Å –ø—É—Å—Ç")
            return
        
        # –û—á–∏—â–∞–µ–º –¥–µ—Ä–µ–≤–æ
        for item in self.corpus_tree.get_children():
            self.corpus_tree.delete(item)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
        for i, pair in enumerate(corpus[:100]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 100 –ø—Ä–∏–º–µ—Ä–æ–≤
            self.corpus_tree.insert('', 'end', values=(
                pair['source'][:100] + '...' if len(pair['source']) > 100 else pair['source'],
                pair['target'][:100] + '...' if len(pair['target']) > 100 else pair['target']
            ))
        
        self.corpus_stats.config(text=f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(corpus)} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
    
    def train_neural_model(self):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        if not hasattr(self, 'training_corpus') or not self.training_corpus:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∫–æ—Ä–ø—É—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return
        
        try:
            epochs = int(self.epochs_entry.get())
            batch_size = int(self.batch_entry.get())
            
            source_texts = [pair['source'] for pair in self.training_corpus]
            target_texts = [pair['target'] for pair in self.training_corpus]
            
            self.training_log.insert(tk.END, f"üéì –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {len(source_texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö...\n")
            
            threading.Thread(target=self._train_neural_thread, 
                           args=(source_texts, target_texts, epochs, batch_size), daemon=True).start()
            
        except ValueError:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —ç–ø–æ—Ö –∏–ª–∏ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞")
    
    def _train_neural_thread(self, source_texts, target_texts, epochs, batch_size):
        """–ü–æ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        try:
            losses = self.translator.train_custom_model(
                source_texts, target_texts, epochs, batch_size
            )
            
            self.root.after(0, lambda: self.training_log.insert(
                tk.END, f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {losses[-1]:.4f}\n"
            ))
            
        except Exception as e:
            self.root.after(0, lambda: self.training_log.insert(
                tk.END, f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}\n"
            ))
    
    def load_training_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        file_path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º",
            filetypes=[("JSON files", "*.json"), ("CSV files", "*.csv"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                if file_path.endswith('.json'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.training_corpus = json.load(f)
                else:  # CSV
                    df = pd.read_csv(file_path)
                    self.training_corpus = df.to_dict('records')
                
                self.training_log.insert(tk.END, f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.training_corpus)} –ø—Ä–∏–º–µ—Ä–æ–≤\n")
                
            except Exception as e:
                messagebox.showerror("–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    
    def evaluate_quality(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        original = self.input_text.get("1.0", tk.END).strip()
        translation = self.output_text.get("1.0", tk.END).strip()
        
        if not original or not translation:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–í–≤–µ–¥–∏—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª –∏ –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            return
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        score = self._calculate_simple_quality(original, translation)
        
        result = f"=== –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê ===\n\n"
        result += f"–û—Ä–∏–≥–∏–Ω–∞–ª: {original}\n"
        result += f"–ü–µ—Ä–µ–≤–æ–¥: {translation}\n\n"
        result += f"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {score:.2%}\n"
        result += f"–î–ª–∏–Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞: {len(original)} —Å–∏–º–≤–æ–ª–æ–≤\n"
        result += f"–î–ª–∏–Ω–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {len(translation)} —Å–∏–º–≤–æ–ª–æ–≤\n"
        
        self.quality_text.delete("1.0", tk.END)
        self.quality_text.insert("1.0", result)
    
    def _calculate_simple_quality(self, original, translation):
        """–ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        length_ratio = len(translation) / max(len(original), 1)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        original_words = set(word_tokenize(original.lower()))
        translation_words = set(word_tokenize(translation.lower()))
        
        common_words = original_words.intersection(translation_words)
        word_similarity = len(common_words) / max(len(original_words), 1)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        score = (min(1.0, length_ratio) * 0.3 + word_similarity * 0.7)
        return score
    
    def compare_models(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞"""
        text = self.input_text.get("1.0", tk.END).strip()
        if not text:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return
        
        results = {}
        
        # Transformer –º–æ–¥–µ–ª–∏
        for model_name, translator in self.translator.transformer_models.items():
            try:
                translation = translator.translate(text)
                results[model_name] = translation
            except Exception as e:
                results[model_name] = f"–û—à–∏–±–∫–∞: {str(e)}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        comparison = "=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===\n\n"
        for model, translation in results.items():
            comparison += f"üîß {model}:\n{translation}\n\n"
            comparison += "-" * 50 + "\n\n"
        
        self.quality_text.delete("1.0", tk.END)
        self.quality_text.insert("1.0", comparison)
    
    def update_models(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        self.training_log.insert(tk.END, "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...\n")
        self.translator.setup_models()
        self.training_log.insert(tk.END, "‚úÖ –ú–æ–¥–µ–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã\n")
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        try:
            import shutil
            cache_dir = Path.home() / '.cache' / 'torch' / 'transformers'
            if cache_dir.exists():
                shutil.rmtree(cache_dir)
            self.system_log.insert(tk.END, "‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω\n")
        except Exception as e:
            self.system_log.insert(tk.END, f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {str(e)}\n")
    
    def show_logs(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –ª–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã"""
        log_content = self.system_log.get("1.0", tk.END)
        messagebox.showinfo("–õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã", log_content)
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            if self.translator.neural_model:
                torch.save(self.translator.neural_model.state_dict(), 'neural_translator.pth')
                self.training_log.insert(tk.END, "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\n")
            else:
                messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            file_path = filedialog.askopenfilename(
                title="–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏",
                filetypes=[("PyTorch files", "*.pth"), ("All files", "*.*")]
            )
            if file_path:
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                self.training_log.insert(tk.END, "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞...")
    print("üéØ –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –°–ò–°–¢–ï–ú–´:")
    print("   ü§ñ Transformer –º–æ–¥–µ–ª–∏ (Helsinki-NLP) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
    print("   üìö –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Tatoeba - –∫—Ä—É–ø–Ω–µ–π—à–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º")
    print("   üß† –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è")
    print("   üåç –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ä")
    print("   üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
    print("   üîß –û–±—É—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    print("   üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    print("   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π")
    print("   üìã –£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º/–≤—Å—Ç–∞–≤–∫–æ–π")
    print("   üìä –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    app = EnhancedTranslationGUI()
    app.root.mainloop()